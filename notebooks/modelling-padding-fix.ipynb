{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch.optim import Adam\n",
    "# becuase we're in a nested folder...\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.preprocess import *\n",
    "from models.AEAD import AEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_v2 =\"../../for_students/data_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Pheobe\\\\Documents\\\\[B]Huawei-Challenge-2020\\\\anomaly-detection-challenge-2020\\\\notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, window_size):\n",
    "    '''\n",
    "    data_dir (str): Base directory of data \n",
    "    window_size (str): Window size for input examples\n",
    "    window_func (str): Window function reference as defined in utils.preprocess\n",
    "                       Option are either 'window' or 'window_func'\n",
    "    '''\n",
    "    train_dir = os.path.join(data_dir, 'training')\n",
    "    train_str = os.path.join(train_dir, 'training_{}.csv')\n",
    "    test_str = os.path.join(data_dir, 'dataset_{}.csv')\n",
    "\n",
    "    train_xs = []\n",
    "    train_ys = []\n",
    "    for i in [1, 2, 3, 4, 5, 100]: # file name updated to v2\n",
    "        train_df_i = pd.read_csv(train_str.format(str(i)))\n",
    "        \n",
    "    # adding padded values and then windowing\n",
    "        #         train_xi = window_func(train_df_i.kpi_value.values, window_size)\n",
    "        local_min = train_df_i.kpi_value[0:window_size].min() # Using the global min as the padding \n",
    "        pad_min = np.ones(window_size) * local_min\n",
    "        x_padded = np.concatenate([pad_min, train_df_i.kpi_value.values])\n",
    "        train_xi = [x_padded[j:j+window_size] for j in range(len(x_padded)-(window_size))]\n",
    "     \n",
    "        train_xs.append(train_xi)\n",
    "        train_ys.append(train_df_i.anomaly_label.values)\n",
    "    x_train = np.concatenate(train_xs)\n",
    "    y_train = np.concatenate(train_ys)\n",
    "    assert len(x_train) == len(y_train)\n",
    "    \n",
    "    test_xs = []\n",
    "    test_ys = []\n",
    "    for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,100,101,102,103,105,106]:  # file name updated to v2\n",
    "        test_df_i = pd.read_csv(test_str.format(str(i)))\n",
    "#         test_xi = window_func(test_df_i.values[:,1], window_size)\n",
    "        test_local_min = test_df_i.kpi_value.values[0:window_size].min() # Using the local min as the padding \n",
    "        test_pad_min = np.ones(window_size) * test_local_min\n",
    "        test_x_padded = np.concatenate([test_pad_min, train_df_i.kpi_value.values])\n",
    "        test_xi = [test_x_padded[j:j+window_size] for j in range(len(test_x_padded)-(window_size))]\n",
    "\n",
    "    test_xs.append(test_xi)\n",
    "    x_test = np.concatenate(test_xs)\n",
    "    print(\"Train_x shape: {}\\nTrain_y shape: {}\\n\\nTest_x shape: {}\".format(x_train.shape, y_train.shape, x_test.shape))\n",
    "    return x_train, y_train, x_test\n",
    "\n",
    "def window_min_max(x):\n",
    "    x_min = x.min(axis=1).reshape(-1, 1)\n",
    "    x_max = x.max(axis=1).reshape(-1, 1)\n",
    "    for i in range(len(x)):\n",
    "        if x_max[i] > x_min[i]:\n",
    "            x[i] =  (x[i] - x_min[i])/(x_max[i] - x_min[i])\n",
    "        else:  # add scenario where x_max = x_min in a window \n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x shape: (54337, 100)\n",
      "Train_y shape: (54337,)\n",
      "\n",
      "Test_x shape: (20159, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test = load_data(data_dir_v2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window level normalisation\n",
    "x_train = window_min_max(x_train)\n",
    "x_test = window_min_max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val incides\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.139521\tTrain auc: 0.555441\n",
      "Train Epoch: 2\tLoss: 0.044801\tTrain auc: 0.573403\n",
      "Train Epoch: 3\tLoss: 0.041435\tTrain auc: 0.621690\n",
      "Train Epoch: 4\tLoss: 0.035134\tTrain auc: 0.709723\n",
      "Train Epoch: 5\tLoss: 0.028953\tTrain auc: 0.754626\n",
      "Train Epoch: 6\tLoss: 0.024517\tTrain auc: 0.757208\n",
      "Train Epoch: 7\tLoss: 0.022002\tTrain auc: 0.755020\n",
      "Train Epoch: 8\tLoss: 0.020749\tTrain auc: 0.755518\n",
      "Train Epoch: 9\tLoss: 0.019927\tTrain auc: 0.760466\n",
      "Train Epoch: 10\tLoss: 0.019314\tTrain auc: 0.763746\n",
      "Train Epoch: 11\tLoss: 0.018788\tTrain auc: 0.765525\n",
      "Train Epoch: 12\tLoss: 0.018336\tTrain auc: 0.768733\n",
      "Train Epoch: 13\tLoss: 0.017964\tTrain auc: 0.770827\n",
      "Train Epoch: 14\tLoss: 0.017664\tTrain auc: 0.771705\n",
      "Train Epoch: 15\tLoss: 0.017421\tTrain auc: 0.773691\n",
      "Train Epoch: 16\tLoss: 0.017225\tTrain auc: 0.774971\n",
      "Train Epoch: 17\tLoss: 0.017054\tTrain auc: 0.777121\n",
      "Train Epoch: 18\tLoss: 0.016907\tTrain auc: 0.778925\n",
      "Train Epoch: 19\tLoss: 0.016762\tTrain auc: 0.780258\n",
      "Train Epoch: 20\tLoss: 0.016617\tTrain auc: 0.780994\n",
      "456\n",
      "0.9070282433898809\n",
      "Train Epoch: 1\tLoss: 0.145182\tTrain auc: 0.621486\n",
      "Train Epoch: 2\tLoss: 0.046829\tTrain auc: 0.622420\n",
      "Train Epoch: 3\tLoss: 0.045723\tTrain auc: 0.627196\n",
      "Train Epoch: 4\tLoss: 0.041102\tTrain auc: 0.636496\n",
      "Train Epoch: 5\tLoss: 0.036323\tTrain auc: 0.660067\n",
      "Train Epoch: 6\tLoss: 0.032381\tTrain auc: 0.681293\n",
      "Train Epoch: 7\tLoss: 0.029947\tTrain auc: 0.683622\n",
      "Train Epoch: 8\tLoss: 0.028641\tTrain auc: 0.681559\n",
      "Train Epoch: 9\tLoss: 0.027659\tTrain auc: 0.680339\n",
      "Train Epoch: 10\tLoss: 0.026934\tTrain auc: 0.678601\n",
      "Train Epoch: 11\tLoss: 0.026400\tTrain auc: 0.678307\n",
      "Train Epoch: 12\tLoss: 0.025974\tTrain auc: 0.678333\n",
      "Train Epoch: 13\tLoss: 0.025607\tTrain auc: 0.678423\n",
      "Train Epoch: 14\tLoss: 0.025272\tTrain auc: 0.679481\n",
      "Train Epoch: 15\tLoss: 0.024969\tTrain auc: 0.680090\n",
      "Train Epoch: 16\tLoss: 0.024689\tTrain auc: 0.681429\n",
      "Train Epoch: 17\tLoss: 0.024430\tTrain auc: 0.683615\n",
      "Train Epoch: 18\tLoss: 0.024183\tTrain auc: 0.683646\n",
      "Train Epoch: 19\tLoss: 0.023935\tTrain auc: 0.683906\n",
      "Train Epoch: 20\tLoss: 0.023687\tTrain auc: 0.685251\n",
      "457\n",
      "0.8288111358399538\n",
      "Train Epoch: 1\tLoss: 0.164890\tTrain auc: 0.726879\n",
      "Train Epoch: 2\tLoss: 0.044375\tTrain auc: 0.728613\n",
      "Train Epoch: 3\tLoss: 0.043289\tTrain auc: 0.728139\n",
      "Train Epoch: 4\tLoss: 0.040920\tTrain auc: 0.701444\n",
      "Train Epoch: 5\tLoss: 0.038643\tTrain auc: 0.691028\n",
      "Train Epoch: 6\tLoss: 0.036234\tTrain auc: 0.685788\n",
      "Train Epoch: 7\tLoss: 0.034308\tTrain auc: 0.693447\n",
      "Train Epoch: 8\tLoss: 0.032703\tTrain auc: 0.688653\n",
      "Train Epoch: 9\tLoss: 0.031494\tTrain auc: 0.680480\n",
      "Train Epoch: 10\tLoss: 0.030628\tTrain auc: 0.675159\n",
      "Train Epoch: 11\tLoss: 0.029919\tTrain auc: 0.672050\n",
      "Train Epoch: 12\tLoss: 0.029299\tTrain auc: 0.671263\n",
      "Train Epoch: 13\tLoss: 0.028763\tTrain auc: 0.669994\n",
      "Train Epoch: 14\tLoss: 0.028304\tTrain auc: 0.669306\n",
      "Train Epoch: 15\tLoss: 0.027900\tTrain auc: 0.671347\n",
      "Train Epoch: 16\tLoss: 0.027514\tTrain auc: 0.672639\n",
      "Train Epoch: 17\tLoss: 0.027159\tTrain auc: 0.673429\n",
      "Train Epoch: 18\tLoss: 0.026819\tTrain auc: 0.674210\n",
      "Train Epoch: 19\tLoss: 0.026502\tTrain auc: 0.677670\n",
      "Train Epoch: 20\tLoss: 0.026198\tTrain auc: 0.680271\n",
      "456\n",
      "0.97178738918182\n",
      "Train Epoch: 1\tLoss: 0.121626\tTrain auc: 0.582862\n",
      "Train Epoch: 2\tLoss: 0.049539\tTrain auc: 0.589811\n",
      "Train Epoch: 3\tLoss: 0.047024\tTrain auc: 0.616155\n",
      "Train Epoch: 4\tLoss: 0.039940\tTrain auc: 0.712921\n",
      "Train Epoch: 5\tLoss: 0.034464\tTrain auc: 0.745140\n",
      "Train Epoch: 6\tLoss: 0.031272\tTrain auc: 0.770492\n",
      "Train Epoch: 7\tLoss: 0.028934\tTrain auc: 0.776472\n",
      "Train Epoch: 8\tLoss: 0.027631\tTrain auc: 0.778190\n",
      "Train Epoch: 9\tLoss: 0.026786\tTrain auc: 0.779010\n",
      "Train Epoch: 10\tLoss: 0.026129\tTrain auc: 0.779974\n",
      "Train Epoch: 11\tLoss: 0.025607\tTrain auc: 0.779896\n",
      "Train Epoch: 12\tLoss: 0.025232\tTrain auc: 0.779711\n",
      "Train Epoch: 13\tLoss: 0.024889\tTrain auc: 0.778744\n",
      "Train Epoch: 14\tLoss: 0.024617\tTrain auc: 0.777971\n",
      "Train Epoch: 15\tLoss: 0.024363\tTrain auc: 0.777643\n",
      "Train Epoch: 16\tLoss: 0.024147\tTrain auc: 0.777285\n",
      "Train Epoch: 17\tLoss: 0.023923\tTrain auc: 0.777691\n",
      "Train Epoch: 18\tLoss: 0.023705\tTrain auc: 0.777356\n",
      "Train Epoch: 19\tLoss: 0.023483\tTrain auc: 0.778670\n",
      "Train Epoch: 20\tLoss: 0.023275\tTrain auc: 0.779474\n",
      "456\n",
      "0.48553444652838507\n",
      "Train Epoch: 1\tLoss: 0.120406\tTrain auc: 0.604371\n",
      "Train Epoch: 2\tLoss: 0.050503\tTrain auc: 0.613134\n",
      "Train Epoch: 3\tLoss: 0.046781\tTrain auc: 0.651405\n",
      "Train Epoch: 4\tLoss: 0.037489\tTrain auc: 0.703850\n",
      "Train Epoch: 5\tLoss: 0.032839\tTrain auc: 0.719651\n",
      "Train Epoch: 6\tLoss: 0.030536\tTrain auc: 0.721813\n",
      "Train Epoch: 7\tLoss: 0.028975\tTrain auc: 0.723514\n",
      "Train Epoch: 8\tLoss: 0.027833\tTrain auc: 0.725605\n",
      "Train Epoch: 9\tLoss: 0.026976\tTrain auc: 0.725780\n",
      "Train Epoch: 10\tLoss: 0.026291\tTrain auc: 0.726757\n",
      "Train Epoch: 11\tLoss: 0.025756\tTrain auc: 0.727980\n",
      "Train Epoch: 12\tLoss: 0.025314\tTrain auc: 0.728679\n",
      "Train Epoch: 13\tLoss: 0.024957\tTrain auc: 0.728970\n",
      "Train Epoch: 14\tLoss: 0.024604\tTrain auc: 0.728933\n",
      "Train Epoch: 15\tLoss: 0.024288\tTrain auc: 0.729513\n",
      "Train Epoch: 16\tLoss: 0.023983\tTrain auc: 0.729773\n",
      "Train Epoch: 17\tLoss: 0.023714\tTrain auc: 0.729284\n",
      "Train Epoch: 18\tLoss: 0.023458\tTrain auc: 0.728409\n",
      "Train Epoch: 19\tLoss: 0.023241\tTrain auc: 0.727826\n",
      "Train Epoch: 20\tLoss: 0.023031\tTrain auc: 0.726848\n",
      "456\n",
      "0.4607272672123109\n",
      "0.7307776964304702\n"
     ]
    }
   ],
   "source": [
    "aead_aucs = []\n",
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "#     x_train_fold = x_train[train_index]\n",
    "#     y_train_fold = y_train[train_index]\n",
    "#     x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "#     y_train_normal = y_train_fold[y_train_fold == 0]\n",
    "\n",
    "#     x_val_fold = x_train[val_index]\n",
    "#     y_val_fold = y_train[val_index]\n",
    "    \n",
    "#     x_train_fold = window_min_max(x_train_fold)\n",
    "#     x_val_fold = window_min_max(x_val_fold)\n",
    "    \n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]  \n",
    "\n",
    "#     aead = AEAD(100,256, 0.0001, 20, 'cpu', Adam).fit(x_train_normal,  y_train_normal)\n",
    "    aead = AEAD(100,256, 0.0001, 20, 'cpu', Adam, normal_only=False).fit(x_train_fold,  y_train_fold)\n",
    "    y_pred_aead = aead.predict(x_val_fold)\n",
    "    val_auc = roc_auc_score(y_val_fold, y_pred_aead)\n",
    "    print(sum(y_val_fold ))\n",
    "    aead_aucs.append(val_auc)\n",
    "    print(val_auc)\n",
    "print(np.mean(aead_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31950022426601254\n",
      "0.5855826880076329\n",
      "0.40918100626886766\n",
      "0.30489379560745794\n",
      "0.14871715669956698\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "\n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    # Train on normal data\n",
    "    iforest = IsolationForest().fit(x_train_normal)\n",
    "    y_pred_iforest = iforest.predict(x_val_fold)\n",
    "\n",
    "    y_pred_iforest[y_pred_iforest==1] = 0\n",
    "    y_pred_iforest[y_pred_iforest==-1] = 1\n",
    "    iforest_f1 = f1_score(y_val_fold, y_pred_iforest, average='macro')\n",
    "    print(iforest_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38577033519720005\n",
      "0.2213723586102315\n",
      "0.37481302496835806\n",
      "0.3713325260196587\n",
      "0.3159978299371823\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "\n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    # Train on normal data\n",
    "    ocsvm = OneClassSVM(gamma='auto').fit(x_train_normal)\n",
    "    # check performance on training set for sanity check. \n",
    "    y_pred_ocsvm = ocsvm.predict(x_val_fold)\n",
    "\n",
    "    y_pred_ocsvm[y_pred_ocsvm==1] = 0\n",
    "    y_pred_ocsvm[y_pred_ocsvm==-1] = 1\n",
    "    ocsvm_f1 = f1_score(y_val_fold, y_pred_ocsvm, average='macro')\n",
    "    print(ocsvm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
