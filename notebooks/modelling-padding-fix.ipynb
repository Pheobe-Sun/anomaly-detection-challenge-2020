{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch.optim import Adam\n",
    "# becuase we're in a nested folder...\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.preprocess import *\n",
    "from models.AEAD import AEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_v2 =\"../for_students/data_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, window_size):\n",
    "    '''\n",
    "    data_dir (str): Base directory of data \n",
    "    window_size (str): Window size for input examples\n",
    "    window_func (str): Window function reference as defined in utils.preprocess\n",
    "                       Option are either 'window' or 'window_func'\n",
    "    '''\n",
    "    train_dir = os.path.join(data_dir, 'training')\n",
    "    train_str = os.path.join(train_dir, 'training_{}.csv')\n",
    "    test_str = os.path.join(data_dir, 'dataset_{}.csv')\n",
    "\n",
    "    train_xs = []\n",
    "    train_ys = []\n",
    "    for i in [1, 2, 3, 4, 5, 100]: # file name updated to v2\n",
    "        train_df_i = pd.read_csv(train_str.format(str(i)))\n",
    "        \n",
    "    # adding padded values and then windowing\n",
    "        #         train_xi = window_func(train_df_i.kpi_value.values, window_size)\n",
    "        local_min = train_df_i.kpi_value[0:window_size].min() # Using the global min as the padding \n",
    "        pad_min = np.ones(window_size) * local_min\n",
    "        x_padded = np.concatenate([pad_min, train_df_i.kpi_value.values])\n",
    "        train_xi = [x_padded[j:j+window_size] for j in range(len(x_padded)-(window_size))]\n",
    "     \n",
    "        train_xs.append(train_xi)\n",
    "        train_ys.append(train_df_i.anomaly_label.values)\n",
    "    x_train = np.concatenate(train_xs)\n",
    "    y_train = np.concatenate(train_ys)\n",
    "    assert len(x_train) == len(y_train)\n",
    "    \n",
    "    test_xs = []\n",
    "    test_ys = []\n",
    "    for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,100,101,102,103,105,106]:  # file name updated to v2\n",
    "        test_df_i = pd.read_csv(test_str.format(str(i)))\n",
    "#         test_xi = window_func(test_df_i.values[:,1], window_size)\n",
    "        test_local_min = test_df_i.kpi_value.values[0:window_size].min() # Using the local min as the padding \n",
    "        test_pad_min = np.ones(window_size) * test_local_min\n",
    "        test_x_padded = np.concatenate([test_pad_min, train_df_i.kpi_value.values])\n",
    "        test_xi = [test_x_padded[j:j+window_size] for j in range(len(test_x_padded)-(window_size))]\n",
    "\n",
    "    test_xs.append(test_xi)\n",
    "    x_test = np.concatenate(test_xs)\n",
    "    print(\"Train_x shape: {}\\nTrain_y shape: {}\\n\\nTest_x shape: {}\".format(x_train.shape, y_train.shape, x_test.shape))\n",
    "    return x_train, y_train, x_test\n",
    "\n",
    "def window_min_max(x):\n",
    "    x_min = x.min(axis=1).reshape(-1, 1)\n",
    "    x_max = x.max(axis=1).reshape(-1, 1)\n",
    "    for i in range(len(x)):\n",
    "        if x_max[i] > x_min[i]:\n",
    "            x[i] =  (x[i] - x_min[i])/(x_max[i] - x_min[i])\n",
    "        else:  # add scenario where x_max = x_min in a window \n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x shape: (54337, 100)\n",
      "Train_y shape: (54337,)\n",
      "\n",
      "Test_x shape: (20159, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test = load_data(data_dir_v2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window level normalisation\n",
    "x_train = window_min_max(x_train)\n",
    "x_test = window_min_max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val incides\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.161638\n",
      "Train Epoch: 2\tLoss: 0.049231\n",
      "Train Epoch: 3\tLoss: 0.045635\n",
      "Train Epoch: 4\tLoss: 0.038351\n",
      "Train Epoch: 5\tLoss: 0.032201\n",
      "Train Epoch: 6\tLoss: 0.028438\n",
      "Train Epoch: 7\tLoss: 0.026221\n",
      "Train Epoch: 8\tLoss: 0.024559\n",
      "Train Epoch: 9\tLoss: 0.023420\n",
      "Train Epoch: 10\tLoss: 0.022658\n",
      "Train Epoch: 11\tLoss: 0.022056\n",
      "Train Epoch: 12\tLoss: 0.021533\n",
      "Train Epoch: 13\tLoss: 0.021053\n",
      "Train Epoch: 14\tLoss: 0.020561\n",
      "Train Epoch: 15\tLoss: 0.020117\n",
      "Train Epoch: 16\tLoss: 0.019747\n",
      "Train Epoch: 17\tLoss: 0.019374\n",
      "Train Epoch: 18\tLoss: 0.019028\n",
      "Train Epoch: 19\tLoss: 0.018746\n",
      "Train Epoch: 20\tLoss: 0.018521\n",
      "456\n",
      "0.8986297440200578\n",
      "Train Epoch: 1\tLoss: 0.192944\n",
      "Train Epoch: 2\tLoss: 0.051935\n",
      "Train Epoch: 3\tLoss: 0.050076\n",
      "Train Epoch: 4\tLoss: 0.044176\n",
      "Train Epoch: 5\tLoss: 0.037955\n",
      "Train Epoch: 6\tLoss: 0.034443\n",
      "Train Epoch: 7\tLoss: 0.032477\n",
      "Train Epoch: 8\tLoss: 0.031437\n",
      "Train Epoch: 9\tLoss: 0.030792\n",
      "Train Epoch: 10\tLoss: 0.030299\n",
      "Train Epoch: 11\tLoss: 0.029848\n",
      "Train Epoch: 12\tLoss: 0.029433\n",
      "Train Epoch: 13\tLoss: 0.029019\n",
      "Train Epoch: 14\tLoss: 0.028602\n",
      "Train Epoch: 15\tLoss: 0.028190\n",
      "Train Epoch: 16\tLoss: 0.027784\n",
      "Train Epoch: 17\tLoss: 0.027410\n",
      "Train Epoch: 18\tLoss: 0.027050\n",
      "Train Epoch: 19\tLoss: 0.026737\n",
      "Train Epoch: 20\tLoss: 0.026454\n",
      "457\n",
      "0.8303784479763556\n",
      "Train Epoch: 1\tLoss: 0.160505\n",
      "Train Epoch: 2\tLoss: 0.049753\n",
      "Train Epoch: 3\tLoss: 0.048284\n",
      "Train Epoch: 4\tLoss: 0.045900\n",
      "Train Epoch: 5\tLoss: 0.042202\n",
      "Train Epoch: 6\tLoss: 0.039741\n",
      "Train Epoch: 7\tLoss: 0.038121\n",
      "Train Epoch: 8\tLoss: 0.036791\n",
      "Train Epoch: 9\tLoss: 0.035710\n",
      "Train Epoch: 10\tLoss: 0.034691\n",
      "Train Epoch: 11\tLoss: 0.033793\n",
      "Train Epoch: 12\tLoss: 0.033105\n",
      "Train Epoch: 13\tLoss: 0.032560\n",
      "Train Epoch: 14\tLoss: 0.032062\n",
      "Train Epoch: 15\tLoss: 0.031562\n",
      "Train Epoch: 16\tLoss: 0.031073\n",
      "Train Epoch: 17\tLoss: 0.030629\n",
      "Train Epoch: 18\tLoss: 0.030226\n",
      "Train Epoch: 19\tLoss: 0.029836\n",
      "Train Epoch: 20\tLoss: 0.029477\n",
      "456\n",
      "0.9721298912924421\n",
      "Train Epoch: 1\tLoss: 0.134841\n",
      "Train Epoch: 2\tLoss: 0.054786\n",
      "Train Epoch: 3\tLoss: 0.052849\n",
      "Train Epoch: 4\tLoss: 0.047004\n",
      "Train Epoch: 5\tLoss: 0.039866\n",
      "Train Epoch: 6\tLoss: 0.035395\n",
      "Train Epoch: 7\tLoss: 0.032791\n",
      "Train Epoch: 8\tLoss: 0.031388\n",
      "Train Epoch: 9\tLoss: 0.030534\n",
      "Train Epoch: 10\tLoss: 0.029876\n",
      "Train Epoch: 11\tLoss: 0.029297\n",
      "Train Epoch: 12\tLoss: 0.028832\n",
      "Train Epoch: 13\tLoss: 0.028381\n",
      "Train Epoch: 14\tLoss: 0.027874\n",
      "Train Epoch: 15\tLoss: 0.027392\n",
      "Train Epoch: 16\tLoss: 0.027010\n",
      "Train Epoch: 17\tLoss: 0.026691\n",
      "Train Epoch: 18\tLoss: 0.026402\n",
      "Train Epoch: 19\tLoss: 0.026143\n",
      "Train Epoch: 20\tLoss: 0.025898\n",
      "456\n",
      "0.4478434162921471\n",
      "Train Epoch: 1\tLoss: 0.127847\n",
      "Train Epoch: 2\tLoss: 0.056032\n",
      "Train Epoch: 3\tLoss: 0.052675\n",
      "Train Epoch: 4\tLoss: 0.044819\n",
      "Train Epoch: 5\tLoss: 0.037773\n",
      "Train Epoch: 6\tLoss: 0.033504\n",
      "Train Epoch: 7\tLoss: 0.031344\n",
      "Train Epoch: 8\tLoss: 0.030224\n",
      "Train Epoch: 9\tLoss: 0.029463\n",
      "Train Epoch: 10\tLoss: 0.028870\n",
      "Train Epoch: 11\tLoss: 0.028368\n",
      "Train Epoch: 12\tLoss: 0.027951\n",
      "Train Epoch: 13\tLoss: 0.027590\n",
      "Train Epoch: 14\tLoss: 0.027280\n",
      "Train Epoch: 15\tLoss: 0.026996\n",
      "Train Epoch: 16\tLoss: 0.026730\n",
      "Train Epoch: 17\tLoss: 0.026509\n",
      "Train Epoch: 18\tLoss: 0.026283\n",
      "Train Epoch: 19\tLoss: 0.026062\n",
      "Train Epoch: 20\tLoss: 0.025865\n",
      "456\n",
      "0.33552189233048035\n",
      "0.6969006783822966\n"
     ]
    }
   ],
   "source": [
    "aead_aucs = []\n",
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "    y_train_normal = y_train_fold[y_train_fold == 0]\n",
    "\n",
    "    \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    x_train_fold = window_min_max(x_train_fold)\n",
    "    x_val_fold = window_min_max(x_val_fold)\n",
    "\n",
    "    aead = AEAD(100,256, 0.0001, 20, 'cpu', Adam).fit(x_train_normal,  y_train_normal)\n",
    "    y_pred_aead = aead.predict(x_val_fold)\n",
    "    val_auc = roc_auc_score(y_val_fold, y_pred_aead)\n",
    "    print(sum(y_val_fold ))\n",
    "    aead_aucs.append(val_auc)\n",
    "    print(val_auc)\n",
    "print(np.mean(aead_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
