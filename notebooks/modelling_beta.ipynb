{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch.optim import Adam\n",
    "# becuase we're in a nested folder...\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.preprocess import *\n",
    "from models.AEAD import AEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAKE AWAY FROM WEEKEND: WINDOW-LEVEL NORMALIZATION PLAYS VERY NICELY WITH NEURAL NETWORKS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =\"../../for_students/data_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, window_size, window_func):\n",
    "    '''\n",
    "    data_dir (str): Base directory of data \n",
    "    window_size (str): Window size for input examples\n",
    "    window_func (str): Window function reference as defined in utils.preprocess\n",
    "                       Option are either 'window' or 'window_func'\n",
    "    '''\n",
    "    train_dir = os.path.join(data_dir, 'training')\n",
    "    train_str = os.path.join(train_dir, 'training_{}.csv')\n",
    "    test_str = os.path.join(data_dir, 'dataset_{}.csv')\n",
    "\n",
    "    train_xs = []\n",
    "    train_ys = []\n",
    "    for i in range(1,4):\n",
    "        train_df_i = pd.read_csv(train_str.format(str(i)))\n",
    "        train_xi = window_func(train_df_i.kpi_value.values, window_size)\n",
    "        train_xs.append(train_xi)\n",
    "        train_ys.append(train_df_i.anomaly_label.values)\n",
    "    x_train = np.concatenate(train_xs)\n",
    "    y_train = np.concatenate(train_ys)\n",
    "    assert len(x_train) == len(y_train)\n",
    "    \n",
    "    test_xs = []\n",
    "    test_ys = []\n",
    "    for i in range(1,7):\n",
    "        test_df_i = pd.read_csv(test_str.format(str(i)))\n",
    "        test_xi = window_func(test_df_i.values[:,1], window_size)\n",
    "        test_xs.append(test_xi)\n",
    "    x_test = np.concatenate(test_xs)\n",
    "    print(\"Train x shape: {}\\nTrain y shape: {}\\n\\nTest x shape: {}\".format(x_train.shape, y_train.shape, x_test.shape))\n",
    "    return x_train, y_train, x_test\n",
    "\n",
    "def window_min_max(x):\n",
    "    x_min = x.min(axis=1).reshape(-1, 1)\n",
    "    x_max = x.max(axis=1).reshape(-1, 1)\n",
    "    for i in range(len(x)):\n",
    "        if x_max[i] > 0:\n",
    "            x[i] =  (x[i] - x_min[i])/(x_max[i] - x_min[i])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x shape: (12096, 100)\n",
      "Train y shape: (12096,)\n",
      "\n",
      "Test x shape: (39476, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test = load_data(data_dir, 100, window_offset)\n",
    "# Window level normalisation\n",
    "x_train = window_min_max(x_train)\n",
    "x_test = window_min_max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val incides\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We may be able to get min max parameters from training set. We should do cross validation also. \n",
    "# minmax_scalar = MinMaxScaler()\n",
    "# minmax_scalar.fit(x_train_normal)\n",
    "# x_train_normal_min_max = minmax_scalar.transform(x_train_normal)\n",
    "# x_train_min_max = minmax_scalar.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44234858809509703\n",
      "0.3060673555749642\n",
      "0.021835826930853213\n",
      "0.44661543494676426\n",
      "0.2661503285140415\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "\n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    # Train on normal data\n",
    "    ocsvm = OneClassSVM(gamma='auto').fit(x_train_normal)\n",
    "    # check performance on training set for sanity check. \n",
    "    y_pred_ocsvm = ocsvm.predict(x_val_fold)\n",
    "\n",
    "    y_pred_ocsvm[y_pred_ocsvm==1] = 0\n",
    "    y_pred_ocsvm[y_pred_ocsvm==-1] = 1\n",
    "    ocsvm_f1 = f1_score(y_val_fold, y_pred_ocsvm, average='macro')\n",
    "    print(ocsvm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019008264462809916\n",
      "0.2583036974704906\n",
      "0.7870261132925992\n",
      "0.2636592729359515\n",
      "0.02223120452708165\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "\n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    # Train on normal data\n",
    "    iforest = IsolationForest().fit(x_train_normal)\n",
    "    y_pred_iforest = iforest.predict(x_val_fold)\n",
    "\n",
    "    y_pred_iforest[y_pred_iforest==1] = 0\n",
    "    y_pred_iforest[y_pred_iforest==-1] = 1\n",
    "    iforest_f1 = f1_score(y_val_fold, y_pred_iforest, average='macro')\n",
    "    print(iforest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal AE (Without finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.210391\n",
      "Train Epoch: 2\tLoss: 0.185857\n",
      "Train Epoch: 3\tLoss: 0.120992\n",
      "Train Epoch: 4\tLoss: 0.074716\n",
      "Train Epoch: 5\tLoss: 0.065574\n",
      "Train Epoch: 6\tLoss: 0.063741\n",
      "Train Epoch: 7\tLoss: 0.063076\n",
      "Train Epoch: 8\tLoss: 0.062695\n",
      "Train Epoch: 9\tLoss: 0.062409\n",
      "Train Epoch: 10\tLoss: 0.062172\n",
      "Train Epoch: 11\tLoss: 0.062006\n",
      "Train Epoch: 12\tLoss: 0.061868\n",
      "Train Epoch: 13\tLoss: 0.061753\n",
      "Train Epoch: 14\tLoss: 0.061636\n",
      "Train Epoch: 15\tLoss: 0.061505\n",
      "Train Epoch: 16\tLoss: 0.061363\n",
      "Train Epoch: 17\tLoss: 0.061200\n",
      "Train Epoch: 18\tLoss: 0.060997\n",
      "Train Epoch: 19\tLoss: 0.060769\n",
      "Train Epoch: 20\tLoss: 0.060507\n",
      "55\n",
      "0.48581203152027663\n",
      "Train Epoch: 1\tLoss: 0.233726\n",
      "Train Epoch: 2\tLoss: 0.211161\n",
      "Train Epoch: 3\tLoss: 0.143856\n",
      "Train Epoch: 4\tLoss: 0.078989\n",
      "Train Epoch: 5\tLoss: 0.069230\n",
      "Train Epoch: 6\tLoss: 0.067862\n",
      "Train Epoch: 7\tLoss: 0.067334\n",
      "Train Epoch: 8\tLoss: 0.066995\n",
      "Train Epoch: 9\tLoss: 0.066754\n",
      "Train Epoch: 10\tLoss: 0.066550\n",
      "Train Epoch: 11\tLoss: 0.066389\n",
      "Train Epoch: 12\tLoss: 0.066218\n",
      "Train Epoch: 13\tLoss: 0.066044\n",
      "Train Epoch: 14\tLoss: 0.065853\n",
      "Train Epoch: 15\tLoss: 0.065646\n",
      "Train Epoch: 16\tLoss: 0.065428\n",
      "Train Epoch: 17\tLoss: 0.065188\n",
      "Train Epoch: 18\tLoss: 0.064950\n",
      "Train Epoch: 19\tLoss: 0.064702\n",
      "Train Epoch: 20\tLoss: 0.064444\n",
      "54\n",
      "0.9985670660089265\n",
      "Train Epoch: 1\tLoss: 0.290304\n",
      "Train Epoch: 2\tLoss: 0.257832\n",
      "Train Epoch: 3\tLoss: 0.169887\n",
      "Train Epoch: 4\tLoss: 0.093488\n",
      "Train Epoch: 5\tLoss: 0.079526\n",
      "Train Epoch: 6\tLoss: 0.077882\n",
      "Train Epoch: 7\tLoss: 0.077522\n",
      "Train Epoch: 8\tLoss: 0.077260\n",
      "Train Epoch: 9\tLoss: 0.077028\n",
      "Train Epoch: 10\tLoss: 0.076845\n",
      "Train Epoch: 11\tLoss: 0.076697\n",
      "Train Epoch: 12\tLoss: 0.076553\n",
      "Train Epoch: 13\tLoss: 0.076398\n",
      "Train Epoch: 14\tLoss: 0.076220\n",
      "Train Epoch: 15\tLoss: 0.076024\n",
      "Train Epoch: 16\tLoss: 0.075794\n",
      "Train Epoch: 17\tLoss: 0.075517\n",
      "Train Epoch: 18\tLoss: 0.075172\n",
      "Train Epoch: 19\tLoss: 0.074773\n",
      "Train Epoch: 20\tLoss: 0.074345\n",
      "54\n",
      "0.9885130373502466\n",
      "Train Epoch: 1\tLoss: 0.231317\n",
      "Train Epoch: 2\tLoss: 0.204835\n",
      "Train Epoch: 3\tLoss: 0.131271\n",
      "Train Epoch: 4\tLoss: 0.078340\n",
      "Train Epoch: 5\tLoss: 0.069335\n",
      "Train Epoch: 6\tLoss: 0.067990\n",
      "Train Epoch: 7\tLoss: 0.067486\n",
      "Train Epoch: 8\tLoss: 0.067144\n",
      "Train Epoch: 9\tLoss: 0.066892\n",
      "Train Epoch: 10\tLoss: 0.066690\n",
      "Train Epoch: 11\tLoss: 0.066525\n",
      "Train Epoch: 12\tLoss: 0.066378\n",
      "Train Epoch: 13\tLoss: 0.066241\n",
      "Train Epoch: 14\tLoss: 0.066091\n",
      "Train Epoch: 15\tLoss: 0.065930\n",
      "Train Epoch: 16\tLoss: 0.065763\n",
      "Train Epoch: 17\tLoss: 0.065578\n",
      "Train Epoch: 18\tLoss: 0.065392\n",
      "Train Epoch: 19\tLoss: 0.065185\n",
      "Train Epoch: 20\tLoss: 0.064957\n",
      "54\n",
      "0.9769086210946677\n",
      "Train Epoch: 1\tLoss: 0.201737\n",
      "Train Epoch: 2\tLoss: 0.180340\n",
      "Train Epoch: 3\tLoss: 0.121317\n",
      "Train Epoch: 4\tLoss: 0.075135\n",
      "Train Epoch: 5\tLoss: 0.065086\n",
      "Train Epoch: 6\tLoss: 0.063040\n",
      "Train Epoch: 7\tLoss: 0.062387\n",
      "Train Epoch: 8\tLoss: 0.062071\n",
      "Train Epoch: 9\tLoss: 0.061821\n",
      "Train Epoch: 10\tLoss: 0.061578\n",
      "Train Epoch: 11\tLoss: 0.061360\n",
      "Train Epoch: 12\tLoss: 0.061194\n",
      "Train Epoch: 13\tLoss: 0.061058\n",
      "Train Epoch: 14\tLoss: 0.060944\n",
      "Train Epoch: 15\tLoss: 0.060829\n",
      "Train Epoch: 16\tLoss: 0.060693\n",
      "Train Epoch: 17\tLoss: 0.060518\n",
      "Train Epoch: 18\tLoss: 0.060305\n",
      "Train Epoch: 19\tLoss: 0.060024\n",
      "Train Epoch: 20\tLoss: 0.059669\n",
      "55\n",
      "0.9998615597600369\n",
      "0.8899324631468308\n"
     ]
    }
   ],
   "source": [
    "aead_aucs = []\n",
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_train_normal = x_train_fold[y_train_fold == 0]\n",
    "    y_train_normal = y_train_fold[y_train_fold == 0]\n",
    "\n",
    "    \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "    \n",
    "    x_train_fold = window_min_max(x_train_fold)\n",
    "    x_val_fold = window_min_max(x_val_fold)\n",
    "\n",
    "    aead = AEAD(100,256, 0.0001, 20, 'cpu', Adam).fit(x_train_normal,  y_train_normal)\n",
    "    y_pred_aead = aead.predict(x_val_fold)\n",
    "    val_auc = roc_auc_score(y_val_fold, y_pred_aead)\n",
    "    print(sum(y_val_fold ))\n",
    "    aead_aucs.append(val_auc)\n",
    "    print(val_auc)\n",
    "print(np.mean(aead_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE with positive penalty (Fancy cost function)\n",
    "\n",
    "## FANCY COST FUNCTION \n",
    "### (I don't know if this exists already, I thought of it while lazing around the house) I think it was inspired by this paper: https://papers.nips.cc/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf ...maybe...anyway, it goes like this:\n",
    "\n",
    "$\\Delta = \\frac{1}{n}\\sum^{n}_{i=1}(X - \\hat{X})$ => i.e. normal Mean squared error used for standard AEs\n",
    "    \n",
    "- where $n$ is the number of examples, $X$ is the input, $\\hat{X}$.\n",
    "\n",
    "$(1-y)\\Delta - y\\Delta$\n",
    "    \n",
    "- where $y$ is the ground truth binary label. \n",
    "\n",
    "This looks a little similar to binary cross entropy but is not for classification. Basically we penalise the autoencoder for recontructing normal examples well. The algorithms below need thresholds for F1 scores but I'll work that out later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.146212\tTrain auc: 0.572925\n",
      "Train Epoch: 2\tLoss: 0.131324\tTrain auc: 0.572902\n",
      "Train Epoch: 3\tLoss: 0.091672\tTrain auc: 0.843739\n",
      "Train Epoch: 4\tLoss: 0.057077\tTrain auc: 0.984190\n",
      "Train Epoch: 5\tLoss: 0.050393\tTrain auc: 0.986628\n",
      "Train Epoch: 6\tLoss: 0.048898\tTrain auc: 0.986744\n",
      "Train Epoch: 7\tLoss: 0.048212\tTrain auc: 0.986908\n",
      "Train Epoch: 8\tLoss: 0.047811\tTrain auc: 0.987046\n",
      "Train Epoch: 9\tLoss: 0.047531\tTrain auc: 0.987078\n",
      "Train Epoch: 10\tLoss: 0.047372\tTrain auc: 0.987086\n",
      "Train Epoch: 11\tLoss: 0.047242\tTrain auc: 0.987068\n",
      "Train Epoch: 12\tLoss: 0.047165\tTrain auc: 0.987076\n",
      "Train Epoch: 13\tLoss: 0.047043\tTrain auc: 0.987007\n",
      "Train Epoch: 14\tLoss: 0.046961\tTrain auc: 0.987020\n",
      "Train Epoch: 15\tLoss: 0.046898\tTrain auc: 0.987120\n",
      "Train Epoch: 16\tLoss: 0.046772\tTrain auc: 0.987177\n",
      "Train Epoch: 17\tLoss: 0.046655\tTrain auc: 0.987416\n",
      "Train Epoch: 18\tLoss: 0.046497\tTrain auc: 0.987644\n",
      "Train Epoch: 19\tLoss: 0.046330\tTrain auc: 0.988289\n",
      "Train Epoch: 20\tLoss: 0.046107\tTrain auc: 0.989094\n",
      "49\n",
      "0.8886029316830064\n",
      "Train Epoch: 1\tLoss: 0.146226\tTrain auc: 0.553786\n",
      "Train Epoch: 2\tLoss: 0.130121\tTrain auc: 0.554188\n",
      "Train Epoch: 3\tLoss: 0.088293\tTrain auc: 0.806322\n",
      "Train Epoch: 4\tLoss: 0.056181\tTrain auc: 0.909507\n",
      "Train Epoch: 5\tLoss: 0.050423\tTrain auc: 0.909290\n",
      "Train Epoch: 6\tLoss: 0.049225\tTrain auc: 0.909140\n",
      "Train Epoch: 7\tLoss: 0.048721\tTrain auc: 0.909284\n",
      "Train Epoch: 8\tLoss: 0.048396\tTrain auc: 0.909218\n",
      "Train Epoch: 9\tLoss: 0.048134\tTrain auc: 0.909053\n",
      "Train Epoch: 10\tLoss: 0.047913\tTrain auc: 0.908748\n",
      "Train Epoch: 11\tLoss: 0.047706\tTrain auc: 0.908538\n",
      "Train Epoch: 12\tLoss: 0.047615\tTrain auc: 0.908363\n",
      "Train Epoch: 13\tLoss: 0.047543\tTrain auc: 0.908155\n",
      "Train Epoch: 14\tLoss: 0.047443\tTrain auc: 0.908068\n",
      "Train Epoch: 15\tLoss: 0.047410\tTrain auc: 0.907813\n",
      "Train Epoch: 16\tLoss: 0.047332\tTrain auc: 0.907904\n",
      "Train Epoch: 17\tLoss: 0.047249\tTrain auc: 0.907748\n",
      "Train Epoch: 18\tLoss: 0.047182\tTrain auc: 0.907825\n",
      "Train Epoch: 19\tLoss: 0.047059\tTrain auc: 0.907801\n",
      "Train Epoch: 20\tLoss: 0.046918\tTrain auc: 0.907960\n",
      "48\n",
      "1.0\n",
      "Train Epoch: 1\tLoss: 0.146481\tTrain auc: 0.559013\n",
      "Train Epoch: 2\tLoss: 0.124302\tTrain auc: 0.559110\n",
      "Train Epoch: 3\tLoss: 0.076145\tTrain auc: 0.844885\n",
      "Train Epoch: 4\tLoss: 0.051472\tTrain auc: 0.912173\n",
      "Train Epoch: 5\tLoss: 0.047418\tTrain auc: 0.913508\n",
      "Train Epoch: 6\tLoss: 0.046654\tTrain auc: 0.913748\n",
      "Train Epoch: 7\tLoss: 0.046282\tTrain auc: 0.914352\n",
      "Train Epoch: 8\tLoss: 0.046068\tTrain auc: 0.914821\n",
      "Train Epoch: 9\tLoss: 0.045915\tTrain auc: 0.915219\n",
      "Train Epoch: 10\tLoss: 0.045814\tTrain auc: 0.915379\n",
      "Train Epoch: 11\tLoss: 0.045714\tTrain auc: 0.915449\n",
      "Train Epoch: 12\tLoss: 0.045668\tTrain auc: 0.915788\n",
      "Train Epoch: 13\tLoss: 0.045582\tTrain auc: 0.915977\n",
      "Train Epoch: 14\tLoss: 0.045551\tTrain auc: 0.916131\n",
      "Train Epoch: 15\tLoss: 0.045484\tTrain auc: 0.916220\n",
      "Train Epoch: 16\tLoss: 0.045384\tTrain auc: 0.916484\n",
      "Train Epoch: 17\tLoss: 0.045280\tTrain auc: 0.916642\n",
      "Train Epoch: 18\tLoss: 0.045169\tTrain auc: 0.916730\n",
      "Train Epoch: 19\tLoss: 0.045037\tTrain auc: 0.916817\n",
      "Train Epoch: 20\tLoss: 0.044854\tTrain auc: 0.916912\n",
      "48\n",
      "0.9798959651342612\n",
      "Train Epoch: 1\tLoss: 0.099001\tTrain auc: 0.719860\n",
      "Train Epoch: 2\tLoss: 0.087408\tTrain auc: 0.720013\n",
      "Train Epoch: 3\tLoss: 0.060608\tTrain auc: 0.852966\n",
      "Train Epoch: 4\tLoss: 0.041195\tTrain auc: 0.932638\n",
      "Train Epoch: 5\tLoss: 0.037126\tTrain auc: 0.934743\n",
      "Train Epoch: 6\tLoss: 0.036364\tTrain auc: 0.935057\n",
      "Train Epoch: 7\tLoss: 0.036101\tTrain auc: 0.935144\n",
      "Train Epoch: 8\tLoss: 0.035950\tTrain auc: 0.935446\n",
      "Train Epoch: 9\tLoss: 0.035929\tTrain auc: 0.935518\n",
      "Train Epoch: 10\tLoss: 0.035887\tTrain auc: 0.935571\n",
      "Train Epoch: 11\tLoss: 0.035846\tTrain auc: 0.935640\n",
      "Train Epoch: 12\tLoss: 0.035802\tTrain auc: 0.935794\n",
      "Train Epoch: 13\tLoss: 0.035762\tTrain auc: 0.935866\n",
      "Train Epoch: 14\tLoss: 0.035729\tTrain auc: 0.935993\n",
      "Train Epoch: 15\tLoss: 0.035661\tTrain auc: 0.935964\n",
      "Train Epoch: 16\tLoss: 0.035609\tTrain auc: 0.936104\n",
      "Train Epoch: 17\tLoss: 0.035547\tTrain auc: 0.936265\n",
      "Train Epoch: 18\tLoss: 0.035467\tTrain auc: 0.936412\n",
      "Train Epoch: 19\tLoss: 0.035371\tTrain auc: 0.936400\n",
      "Train Epoch: 20\tLoss: 0.035302\tTrain auc: 0.936433\n",
      "48\n",
      "0.9739649233797272\n",
      "Train Epoch: 1\tLoss: 0.074757\tTrain auc: 0.795838\n",
      "Train Epoch: 2\tLoss: 0.068443\tTrain auc: 0.795993\n",
      "Train Epoch: 3\tLoss: 0.053270\tTrain auc: 0.837928\n",
      "Train Epoch: 4\tLoss: 0.035815\tTrain auc: 0.936611\n",
      "Train Epoch: 5\tLoss: 0.032041\tTrain auc: 0.939133\n",
      "Train Epoch: 6\tLoss: 0.031531\tTrain auc: 0.939293\n",
      "Train Epoch: 7\tLoss: 0.031359\tTrain auc: 0.939224\n",
      "Train Epoch: 8\tLoss: 0.031242\tTrain auc: 0.939255\n",
      "Train Epoch: 9\tLoss: 0.031173\tTrain auc: 0.939270\n",
      "Train Epoch: 10\tLoss: 0.031120\tTrain auc: 0.939232\n",
      "Train Epoch: 11\tLoss: 0.031055\tTrain auc: 0.939256\n",
      "Train Epoch: 12\tLoss: 0.031030\tTrain auc: 0.939296\n",
      "Train Epoch: 13\tLoss: 0.031033\tTrain auc: 0.939326\n",
      "Train Epoch: 14\tLoss: 0.030997\tTrain auc: 0.939402\n",
      "Train Epoch: 15\tLoss: 0.030913\tTrain auc: 0.939448\n",
      "Train Epoch: 16\tLoss: 0.030827\tTrain auc: 0.939534\n",
      "Train Epoch: 17\tLoss: 0.030682\tTrain auc: 0.939639\n",
      "Train Epoch: 18\tLoss: 0.030545\tTrain auc: 0.939828\n",
      "Train Epoch: 19\tLoss: 0.030363\tTrain auc: 0.939926\n",
      "Train Epoch: 20\tLoss: 0.030144\tTrain auc: 0.940359\n",
      "48\n",
      "0.9967928440882889\n",
      "0.9678513328570568\n"
     ]
    }
   ],
   "source": [
    "aead_aucs = []\n",
    "for train_index, val_index in skf.split(x_train, y_train):\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    aead = AEAD(100,256, 0.0001, 20, 'cpu', Adam, normal_only=False).fit(x_train_fold,  y_train_fold)\n",
    "    y_pred_aead = aead.predict(x_val_fold)\n",
    "    val_auc = roc_auc_score(y_val_fold, y_pred_aead)\n",
    "    print(sum(y_val_fold ))\n",
    "    aead_aucs.append(val_auc)\n",
    "    print(val_auc)\n",
    "print(np.mean(aead_aucs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
